"""
Agents endpoint – the main homepage API.

Lists detected agents (prompt templates), their score / cost / latency analytics,
suggestions generated by the prompt-tuning pipeline, and exposes manual triggers
for the three Celery jobs (agent discovery, judge scoring, prompt tuning).
"""

import logging
import uuid as _uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from sqlalchemy import and_, cast, func, select, Float
from sqlalchemy.ext.asyncio import AsyncSession

from overmind_core.api.v1.endpoints.jobs import JobOut
from overmind_core.api.v1.endpoints.utils.agents import (
    humanise_slug,
    get_analytics_for_prompt,
    get_latest_prompts_for_project,
)
from overmind_core.tasks.periodic_reviews import REVIEW_THRESHOLDS
from overmind_core.api.v1.endpoints.utils.jobs import sync_running_job_statuses
from overmind_core.api.v1.helpers.authentication import AuthenticatedUserOrToken, get_current_user
from overmind_core.db.session import get_db
from overmind_core.models.jobs import Job
from overmind_core.models.prompts import Prompt
from overmind_core.models.suggestions import Suggestion
from overmind_core.models.traces import SpanModel

logger = logging.getLogger(__name__)
router = APIRouter()


# ---------------------------------------------------------------------------
# Pydantic response models
# ---------------------------------------------------------------------------


class HourlyBucket(BaseModel):
    hour: str  # ISO-8601 hour string e.g. "2026-02-10T14:00:00Z"
    avg_score: Optional[float] = None
    span_count: int = 0
    avg_latency_ms: Optional[float] = None
    estimated_cost: float = 0.0


class AgentAnalytics(BaseModel):
    total_spans: int = 0
    scored_spans: int = 0
    avg_score: Optional[float] = None
    avg_latency_ms: Optional[float] = None
    total_estimated_cost: float = 0.0
    hourly: List[HourlyBucket] = []


class SuggestionOut(BaseModel):
    id: str
    title: str
    description: str
    new_prompt_version: Optional[int] = None
    new_prompt_text: Optional[str] = None
    scores: Optional[Dict[str, Any]] = None
    status: str = "pending"
    vote: int = 0
    feedback: Optional[str] = None
    created_at: Optional[str] = None


class AgentOut(BaseModel):
    slug: str
    name: str  # display name (slug humanised)
    prompt_id: str
    version: int
    analytics: AgentAnalytics
    suggestions: List[SuggestionOut] = []
    jobs: List[JobOut] = []
    ready_for_review: bool = False
    tags: List[str] = []


class AgentsResponse(BaseModel):
    data: List[AgentOut]


class TriggerResponse(BaseModel):
    message: str
    job_id: str
    celery_task_id: Optional[str] = None


# ---------------------------------------------------------------------------
# Endpoints
# ---------------------------------------------------------------------------


@router.get("/", response_model=AgentsResponse)
async def list_agents(
    project_id: Optional[str] = Query(None, description="Filter by project ID"),
    user: AuthenticatedUserOrToken = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    List all detected agents (prompt templates) with analytics and suggestions.
    If project_id is not provided, uses the first project the user belongs to.
    """
    if project_id:
        pid = _uuid.UUID(project_id)
        if not await user.is_project_member(pid, db):
            raise HTTPException(status_code=403, detail="Access denied to this project")
    elif user.user.projects:
        pid = user.user.projects[0].project_id
    else:
        return AgentsResponse(data=[])

    # Reconcile stale 'running' job statuses with Celery backend
    await sync_running_job_statuses(db, pid)

    prompts = await get_latest_prompts_for_project(pid, db)

    agents: List[AgentOut] = []
    for prompt in prompts:
        # Analytics
        analytics_dict = await get_analytics_for_prompt(prompt.prompt_id, pid, db)
        analytics = AgentAnalytics(**analytics_dict)

        # Suggestions from DB
        sugg_q = await db.execute(
            select(Suggestion)
            .where(
                and_(
                    Suggestion.prompt_slug == prompt.slug,
                    Suggestion.project_id == pid,
                )
            )
            .order_by(Suggestion.created_at.desc())
            .limit(10)
        )
        suggestions = [
            SuggestionOut(
                id=str(s.suggestion_id),
                title=s.title,
                description=s.description,
                new_prompt_version=s.new_prompt_version,
                new_prompt_text=s.new_prompt_text,
                scores=s.scores,
                status=s.status,
                vote=s.vote,
                feedback=s.feedback,
                created_at=s.created_at.isoformat() if s.created_at else None,
            )
            for s in sugg_q.scalars().all()
        ]

        # Recent jobs for this prompt
        jobs_q = await db.execute(
            select(Job)
            .where(
                and_(
                    Job.project_id == pid,
                    Job.prompt_slug == prompt.slug,
                )
            )
            .order_by(Job.created_at.desc())
            .limit(5)
        )
        agent_display_name = prompt.display_name or humanise_slug(prompt.slug)
        jobs = [
            JobOut.from_model(j, prompt_display_name=agent_display_name)
            for j in jobs_q.scalars().all()
        ]

        agent_desc = prompt.agent_description or {}
        next_review_span_count = agent_desc.get(
            "next_review_span_count", REVIEW_THRESHOLDS[0]
        )
        initial_review_due = (
            not agent_desc.get("initial_review_completed")
            and analytics.scored_spans >= REVIEW_THRESHOLDS[0]
        )
        periodic_review_due = (
            bool(agent_desc.get("initial_review_completed"))
            and analytics.scored_spans >= next_review_span_count
        )
        ready_for_review = bool(
            agent_desc.get("description")
            and prompt.evaluation_criteria
            and (initial_review_due or periodic_review_due)
        )

        agents.append(
            AgentOut(
                slug=prompt.slug,
                name=agent_display_name,
                prompt_id=prompt.prompt_id,
                version=prompt.version,
                analytics=analytics,
                suggestions=suggestions,
                jobs=jobs,
                ready_for_review=ready_for_review,
                tags=prompt.tags or [],
            )
        )

    return AgentsResponse(data=agents)


# ---------------------------------------------------------------------------
# Agent detail
# ---------------------------------------------------------------------------


class PromptVersionOut(BaseModel):
    prompt_id: str
    slug: str
    version: int
    prompt_text: str
    hash: str
    evaluation_criteria: Optional[Dict[str, Any]] = None
    improvement_metadata: Optional[Dict[str, Any]] = None
    created_at: Optional[str] = None
    # Analytics for this specific version
    total_spans: int = 0
    scored_spans: int = 0
    avg_score: Optional[float] = None
    avg_latency_ms: Optional[float] = None


class AgentDetailOut(BaseModel):
    slug: str
    name: str
    project_id: str
    latest_version: int
    analytics: AgentAnalytics
    versions: List[PromptVersionOut] = []
    suggestions: List[SuggestionOut] = []
    jobs: List[JobOut] = []
    agent_description: Optional[Dict[str, Any]] = None
    ready_for_review: bool = False
    tags: List[str] = []


@router.get("/{prompt_slug}/detail", response_model=AgentDetailOut)
async def get_agent_detail(
    prompt_slug: str,
    project_id: Optional[str] = Query(None),
    user: AuthenticatedUserOrToken = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    Get full agent detail — all prompt versions, their texts, scores,
    version numbers, suggestions, and job statuses.
    """
    if project_id:
        pid = _uuid.UUID(project_id)
        if not await user.is_project_member(pid, db):
            raise HTTPException(status_code=403, detail="Access denied to this project")
    elif user.user.projects:
        pid = user.user.projects[0].project_id
    else:
        raise HTTPException(status_code=400, detail="No project found for user")

    # Sync running jobs
    await sync_running_job_statuses(db, pid)

    # Get all versions of this prompt
    versions_q = await db.execute(
        select(Prompt)
        .where(and_(Prompt.slug == prompt_slug, Prompt.project_id == pid))
        .order_by(Prompt.version.desc())
    )
    all_versions = versions_q.scalars().all()

    if not all_versions:
        raise HTTPException(status_code=404, detail="Agent not found")

    latest = all_versions[0]

    # Build per-version analytics
    version_outs: List[PromptVersionOut] = []
    for v in all_versions:
        # Total spans for this version (exclude system-generated spans)
        total_q = await db.execute(
            select(func.count(SpanModel.span_id)).where(
                and_(
                    SpanModel.prompt_id == v.prompt_id,
                    SpanModel.exclude_system_spans(),
                )
            )
        )
        total_spans = total_q.scalar() or 0

        # Scored spans + avg score
        scored_q = await db.execute(
            select(
                func.count(SpanModel.span_id),
                func.avg(cast(SpanModel.feedback_score["correctness"], Float)),
            ).where(
                and_(
                    SpanModel.prompt_id == v.prompt_id,
                    SpanModel.feedback_score.has_key("correctness"),
                    SpanModel.exclude_system_spans(),
                )
            )
        )
        scored_row = scored_q.one()
        scored_spans = scored_row[0] or 0
        avg_score = (
            round(float(scored_row[1]), 4) if scored_row[1] is not None else None
        )

        # Avg latency
        lat_q = await db.execute(
            select(
                func.avg(
                    (SpanModel.end_time_unix_nano - SpanModel.start_time_unix_nano)
                    / 1_000_000.0
                )
            ).where(
                and_(
                    SpanModel.prompt_id == v.prompt_id,
                    SpanModel.exclude_system_spans(),
                )
            )
        )
        avg_lat = lat_q.scalar()
        if avg_lat is not None:
            avg_lat = round(float(avg_lat), 2)

        version_outs.append(
            PromptVersionOut(
                prompt_id=v.prompt_id,
                slug=v.slug,
                version=v.version,
                prompt_text=v.prompt,
                hash=v.hash,
                evaluation_criteria=v.evaluation_criteria,
                improvement_metadata=v.improvement_metadata,
                created_at=v.created_at.isoformat() if v.created_at else None,
                total_spans=total_spans,
                scored_spans=scored_spans,
                avg_score=avg_score,
                avg_latency_ms=avg_lat,
            )
        )

    # Overall analytics (across latest version)
    analytics_dict = await get_analytics_for_prompt(latest.prompt_id, pid, db)
    analytics = AgentAnalytics(**analytics_dict)

    # Suggestions
    sugg_q = await db.execute(
        select(Suggestion)
        .where(
            and_(Suggestion.prompt_slug == prompt_slug, Suggestion.project_id == pid)
        )
        .order_by(Suggestion.created_at.desc())
        .limit(20)
    )
    suggestions = [
        SuggestionOut(
            id=str(s.suggestion_id),
            title=s.title,
            description=s.description,
            new_prompt_version=s.new_prompt_version,
            new_prompt_text=s.new_prompt_text,
            scores=s.scores,
            status=s.status,
            vote=s.vote,
            feedback=s.feedback,
            created_at=s.created_at.isoformat() if s.created_at else None,
        )
        for s in sugg_q.scalars().all()
    ]

    # Jobs
    jobs_q = await db.execute(
        select(Job)
        .where(
            and_(
                Job.project_id == pid,
                Job.prompt_slug == prompt_slug,
            )
        )
        .order_by(Job.created_at.desc())
        .limit(20)
    )
    detail_display_name = latest.display_name or humanise_slug(latest.slug)
    jobs = [
        JobOut.from_model(j, prompt_display_name=detail_display_name)
        for j in jobs_q.scalars().all()
    ]

    agent_desc = latest.agent_description or {}
    next_review_span_count = agent_desc.get(
        "next_review_span_count", REVIEW_THRESHOLDS[0]
    )
    initial_review_due = (
        not agent_desc.get("initial_review_completed")
        and analytics.scored_spans >= REVIEW_THRESHOLDS[0]
    )
    periodic_review_due = (
        bool(agent_desc.get("initial_review_completed"))
        and analytics.scored_spans >= next_review_span_count
    )
    ready_for_review = bool(
        agent_desc.get("description")
        and latest.evaluation_criteria
        and (initial_review_due or periodic_review_due)
    )

    return AgentDetailOut(
        slug=latest.slug,
        name=detail_display_name,
        project_id=str(pid),
        latest_version=latest.version,
        analytics=analytics,
        versions=version_outs,
        suggestions=suggestions,
        jobs=jobs,
        agent_description=latest.agent_description,
        ready_for_review=ready_for_review,
        tags=latest.tags or [],
    )


# ---------------------------------------------------------------------------
# Agent metadata update (name + tags)
# ---------------------------------------------------------------------------


class UpdateAgentMetadataRequest(BaseModel):
    name: Optional[str] = None
    tags: Optional[List[str]] = None


class UpdateAgentMetadataResponse(BaseModel):
    slug: str
    name: str
    tags: List[str]


@router.put("/{prompt_slug}/metadata", response_model=UpdateAgentMetadataResponse)
async def update_agent_metadata(
    prompt_slug: str,
    request: UpdateAgentMetadataRequest,
    project_id: Optional[str] = Query(None),
    user: AuthenticatedUserOrToken = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    Update a user-facing name and/or tags for an agent (all versions of the slug).

    - **name**: Custom display name (3–255 characters). Overwrites the auto-generated one.
    - **tags**: List of category strings, e.g. ["HR", "financial"]. Replaces existing tags.
    """
    if project_id:
        pid = _uuid.UUID(project_id)
    elif user.user.projects:
        pid = user.user.projects[0].project_id
    else:
        raise HTTPException(status_code=400, detail="No project found for user")

    # Validate name if provided
    if request.name is not None:
        name_stripped = request.name.strip()
        if len(name_stripped) < 3:
            raise HTTPException(
                status_code=400, detail="Name must be at least 3 characters long"
            )
        if len(name_stripped) > 255:
            raise HTTPException(
                status_code=400, detail="Name must be no more than 255 characters"
            )
    else:
        name_stripped = None

    # Validate tags if provided
    if request.tags is not None:
        cleaned_tags = [t.strip() for t in request.tags if t.strip()]
        if len(cleaned_tags) > 20:
            raise HTTPException(
                status_code=400, detail="Maximum 20 tags allowed per agent"
            )
        for tag in cleaned_tags:
            if len(tag) > 50:
                raise HTTPException(
                    status_code=400, detail=f"Tag '{tag}' exceeds 50 character limit"
                )
    else:
        cleaned_tags = None

    # Fetch all versions of this slug in the project
    versions_q = await db.execute(
        select(Prompt).where(and_(Prompt.slug == prompt_slug, Prompt.project_id == pid))
    )
    all_versions = versions_q.scalars().all()

    if not all_versions:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Check project membership
    if not await user.is_project_member(pid, db):
        raise HTTPException(
            status_code=403,
            detail="Access denied: User is not a member of this project",
        )

    # Apply updates across all versions so the change is version-independent
    for prompt in all_versions:
        if name_stripped is not None:
            prompt.display_name = name_stripped
        if cleaned_tags is not None:
            prompt.tags = cleaned_tags

    await db.commit()

    latest = max(all_versions, key=lambda p: p.version)
    final_name = latest.display_name or humanise_slug(latest.slug)
    final_tags = latest.tags or []

    return UpdateAgentMetadataResponse(
        slug=prompt_slug,
        name=final_name,
        tags=final_tags,
    )
